{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.415139Z",
     "start_time": "2024-05-19T07:30:47.411615Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.465966Z",
     "start_time": "2024-05-19T07:30:47.425145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "# Add \".jpg\" extension to all filenames\n",
    "df['Image name'] = df['Image name'].astype(str) + '.png'\n",
    "df.head()"
   ],
   "id": "ab1f3826ec10e3ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                              Image name  Shirt  Vintage  Sweatshirt  T-shirt  \\\n",
       "0  91da8ace712f42f7bac3bd4c9803ec020.png      0        0           0        0   \n",
       "1  b69efd7be15c4384bcd4642205d01cc11.png      0        1           0        0   \n",
       "2  4c48eea103eb437e909b9fe5f25647222.png      0        0           0        0   \n",
       "3  5aec4aa440564f339f95ebf5d4ab913c3.png      0        0           0        0   \n",
       "4  6d79eae68ac24997941fa950ff68ac844.png      0        0           0        0   \n",
       "\n",
       "   Long Sleeve  Polo  Floral  Camo  Striped  ...  Skirt  Mini skirt  \\\n",
       "0            0     0       1     0        0  ...      0           0   \n",
       "1            0     0       1     0        0  ...      0           0   \n",
       "2            0     0       1     0        0  ...      0           0   \n",
       "3            0     0       1     0        0  ...      0           0   \n",
       "4            0     0       1     0        0  ...      0           0   \n",
       "\n",
       "   Midi skirt  Maxi skirt  Dress  Mini dress  Midi dress  Maxi dress  \\\n",
       "0           0           0      0           0           0           0   \n",
       "1           0           0      0           0           0           0   \n",
       "2           0           0      0           0           0           0   \n",
       "3           0           0      0           0           0           0   \n",
       "4           0           0      0           0           0           0   \n",
       "\n",
       "   Turtleneck  Leather  \n",
       "0           0        0  \n",
       "1           0        0  \n",
       "2           0        0  \n",
       "3           0        0  \n",
       "4           0        0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image name</th>\n",
       "      <th>Shirt</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Sweatshirt</th>\n",
       "      <th>T-shirt</th>\n",
       "      <th>Long Sleeve</th>\n",
       "      <th>Polo</th>\n",
       "      <th>Floral</th>\n",
       "      <th>Camo</th>\n",
       "      <th>Striped</th>\n",
       "      <th>...</th>\n",
       "      <th>Skirt</th>\n",
       "      <th>Mini skirt</th>\n",
       "      <th>Midi skirt</th>\n",
       "      <th>Maxi skirt</th>\n",
       "      <th>Dress</th>\n",
       "      <th>Mini dress</th>\n",
       "      <th>Midi dress</th>\n",
       "      <th>Maxi dress</th>\n",
       "      <th>Turtleneck</th>\n",
       "      <th>Leather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91da8ace712f42f7bac3bd4c9803ec020.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b69efd7be15c4384bcd4642205d01cc11.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c48eea103eb437e909b9fe5f25647222.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5aec4aa440564f339f95ebf5d4ab913c3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6d79eae68ac24997941fa950ff68ac844.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.479204Z",
     "start_time": "2024-05-19T07:30:47.466972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sum the occurrences of each label (excluding the 'Image name' column)\n",
    "def label_counts(df):\n",
    "    return df.drop(columns=['Image name']).sum()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split into train and temporary set\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
    "\n",
    "# Step 2: Split the temp set into validation and test\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 15% val, 15% test\n",
    "\n",
    "print(label_counts(df))\n",
    "# print(label_counts(train_df))\n",
    "# print(label_counts(val_df))\n",
    "# print(label_counts(test_df))"
   ],
   "id": "9592e44000c70080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shirt          2594\n",
      "Vintage        6876\n",
      "Sweatshirt      402\n",
      "T-shirt        1364\n",
      "Long Sleeve     857\n",
      "               ... \n",
      "Mini dress      205\n",
      "Midi dress      201\n",
      "Maxi dress      209\n",
      "Turtleneck       37\n",
      "Leather         553\n",
      "Length: 66, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.485190Z",
     "start_time": "2024-05-19T07:30:47.481209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ],
   "id": "676652d364ff00de",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.506602Z",
     "start_time": "2024-05-19T07:30:47.503798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Weight Transforms\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ],
   "id": "f76e9066ff94017d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.544050Z",
     "start_time": "2024-05-19T07:30:47.538122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "Image.init() # reinitialize the image library to clear the cache\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.df.iloc[idx, 0]  # Assuming the first column is 'Image name'\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Force initial conversion to RGB\n",
    "        image = image.convert('RGB') \n",
    "        \n",
    "        # Check for palette mode and transparency (even after RGB conversion)\n",
    "        if image.mode == 'P' and image.info.get(\"transparency\", None) is not None:\n",
    "            image = image.convert('RGBA')  # Convert to RGBA if still needed\n",
    "        \n",
    "        # Use the preprocess transform\n",
    "        image = preprocess(image)\n",
    "        image.requires_grad = True\n",
    "\n",
    "        # Convert labels to numeric format (int64 in this example)\n",
    "        labels = torch.tensor(self.df.iloc[idx, 1:].values.astype(np.int64), dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image) \n",
    "\n",
    "        return image, labels"
   ],
   "id": "16e8d82e59f72ba3",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.575253Z",
     "start_time": "2024-05-19T07:30:47.571930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ClothingDataset(train_df, 'images', transform=None)  # No additional transforms\n",
    "val_dataset = ClothingDataset(val_df, 'images', transform=None)\n",
    "test_dataset = ClothingDataset(test_df, 'images', transform=None)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "4f1ddfd1578cfdf9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.938888Z",
     "start_time": "2024-05-19T07:30:47.685074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import resnet50\n",
    "# Load the pre-trained model\n",
    "resnet50 = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Modify for your number of classes (28 in your example)\n",
    "num_classes = 66 \n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(resnet50.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()  # Sigmoid for multi-label\n",
    ")"
   ],
   "id": "959bc3d3a84240d0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.943248Z",
     "start_time": "2024-05-19T07:30:47.939891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False"
   ],
   "id": "9c07bee87eecd7db",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.951383Z",
     "start_time": "2024-05-19T07:30:47.944254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for multi-label\n",
    "optimizer = optim.Adam(resnet50.fc.parameters(), lr=0.001)  # Only optimize the last layer"
   ],
   "id": "417c5a9ec7b8a666",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:30:47.956369Z",
     "start_time": "2024-05-19T07:30:47.952386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "686cca87d59f035e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T07:56:03.599615Z",
     "start_time": "2024-05-19T07:30:47.957374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss\n",
    "\n",
    "\n",
    "resnet50.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize with a high value\n",
    "patience = 5  # Number of epochs to wait before stopping\n",
    "epochs_without_improvement = 0\n",
    "best_model_state = None  # To store the best model's state_dict\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet50.train()  # Set the model to training mode\n",
    "    \n",
    "    # Training Loop\n",
    "    train_loss = 0.0  # Initialize training loss for the epoch\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet50(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)  # Average training loss\n",
    "\n",
    "    # Validation Loop (Evaluation Mode)\n",
    "    resnet50.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0  # Initialize validation loss\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # No gradients needed during evaluation\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = resnet50(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Convert outputs to binary predictions (threshold at 0.5)\n",
    "            preds = (outputs > 0.5).float() \n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())  # Accumulate predictions\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)  # Average validation loss\n",
    "    scheduler.step()  # Update learning rate after the epoch\n",
    "    \n",
    "    # Concatenate predictions and labels\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  \n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    hamming = hamming_loss(all_labels, all_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.4f}, Val F1 (micro): {f1_micro:.4f}, Val F1 (macro): {f1_macro:.4f}, Val Hamming Loss: {hamming:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = resnet50.state_dict()\n",
    "        torch.save(best_model_state, 'best_model.pth')\n",
    "        print(f'New best model saved with validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "    # Early stopping (optional) \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break  # Stop training"
   ],
   "id": "25072d5c25c56307",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasper\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6955, Val Loss: 0.6952, Val Accuracy: 0.0000, Val F1 (micro): 0.1081, Val F1 (macro): 0.0926, Val Hamming Loss: 0.4954\n",
      "New best model saved with validation loss: 0.6952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasper\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.6955, Val Loss: 0.6963, Val Accuracy: 0.0000, Val F1 (micro): 0.1071, Val F1 (macro): 0.0930, Val Hamming Loss: 0.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasper\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Training Loop\u001B[39;00m\n\u001B[0;32m     19\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m  \u001B[38;5;66;03m# Initialize training loss for the epoch\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[18], line 22\u001B[0m, in \u001B[0;36mClothingDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     19\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(image_path)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Force initial conversion to RGB\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Check for palette mode and transparency (even after RGB conversion)\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m image\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\PIL\\Image.py:934\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    932\u001B[0m         mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGBA\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    933\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mor\u001B[39;00m (mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m matrix):\n\u001B[1;32m--> 934\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    936\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m matrix:\n\u001B[0;32m    937\u001B[0m     \u001B[38;5;66;03m# matrix conversion\u001B[39;00m\n\u001B[0;32m    938\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\PycharmProjects\\preloved-auto-tag\\.venv\\Lib\\site-packages\\PIL\\Image.py:1192\u001B[0m, in \u001B[0;36mImage.copy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1184\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1185\u001B[0m \u001B[38;5;124;03mCopies this image. Use this method if you wish to paste things\u001B[39;00m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;124;03minto an image, but still retain the original.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1189\u001B[0m \u001B[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m-> 1192\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "21180e21ec8bf2cf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
